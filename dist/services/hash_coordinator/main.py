"""
Hash Coordinator Service - –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π —Ö–µ—à–µ–π

–§—É–Ω–∫—Ü–∏–∏:
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ chunk batches –¥–ª—è –≤–æ—Ä–∫–µ—Ä–æ–≤
- –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–∞ —á–∞–Ω–∫–æ–≤ –ø–æ–¥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
- –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ —á–µ—Ä–µ–∑ gossip
- –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ orphaned chunks
- –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ batches
"""

import asyncio
import hashlib
import json
import logging
import statistics
import string
import time
import csv
import os
from collections import defaultdict
from dataclasses import dataclass, asdict
from datetime import datetime
from typing import Dict, List, Optional, Any

from layers.service import BaseService, service_method

# Import PCAP parser
try:
    from .pcap_parser import PCAPParser, parse_hccapx, parse_22000
except ImportError:
    PCAPParser = None
    parse_hccapx = None
    parse_22000 = None


@dataclass
class ChunkInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —á–∞–Ω–∫–µ"""
    chunk_id: int
    start_index: int
    end_index: int
    chunk_size: int
    assigned_worker: str
    status: str  # assigned, working, solved, timeout
    priority: int
    created_at: float

    def to_dict(self) -> dict:
        return asdict(self)


@dataclass
class BatchInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ batch —á–∞–Ω–∫–æ–≤"""
    version: int
    chunks: List[ChunkInfo]
    created_at: float
    is_recovery: bool = False

    def to_dict(self) -> dict:
        return {
            "version": self.version,
            "chunks": [c.to_dict() for c in self.chunks],
            "created_at": self.created_at,
            "is_recovery": self.is_recovery
        }


class PerformanceAnalyzer:
    """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤–æ—Ä–∫–µ—Ä–æ–≤"""

    def __init__(self, base_chunk_size: int = 1_000_000):
        self.base_chunk_size = base_chunk_size
        self.worker_speeds: Dict[str, float] = {}  # {worker_id: hashes/sec}
        self.worker_history: Dict[str, List[Dict]] = defaultdict(list)

    def update_worker_performance(self, worker_id: str, chunk_size: int, time_taken: float):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –≤–æ—Ä–∫–µ—Ä–∞"""
        if time_taken <= 0:
            return

        hash_rate = chunk_size / time_taken
        self.worker_speeds[worker_id] = hash_rate

        self.worker_history[worker_id].append({
            "chunk_size": chunk_size,
            "time_taken": time_taken,
            "hash_rate": hash_rate,
            "timestamp": time.time()
        })

        # –•—Ä–∞–Ω–∏–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 –∑–∞–ø–∏—Å–µ–π
        if len(self.worker_history[worker_id]) > 10:
            self.worker_history[worker_id] = self.worker_history[worker_id][-10:]

    def calculate_cluster_stats(self) -> dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä—É"""
        if not self.worker_speeds:
            return {
                "avg_speed": 0,
                "median_speed": 0,
                "total_speed": 0,
                "min_speed": 0,
                "max_speed": 0,
                "std_dev": 0
            }

        speeds = list(self.worker_speeds.values())

        return {
            "avg_speed": statistics.mean(speeds),
            "median_speed": statistics.median(speeds),
            "total_speed": sum(speeds),
            "min_speed": min(speeds),
            "max_speed": max(speeds),
            "std_dev": statistics.stdev(speeds) if len(speeds) > 1 else 0
        }

    def calculate_adaptive_chunk_size(self, worker_id: str) -> int:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è –≤–æ—Ä–∫–µ—Ä–∞"""
        worker_speed = self.worker_speeds.get(worker_id, 0)

        if worker_speed == 0:
            return self.base_chunk_size

        stats = self.calculate_cluster_stats()
        avg_speed = stats["avg_speed"]

        if avg_speed == 0:
            return self.base_chunk_size

        # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç: worker_speed / avg_speed
        speed_ratio = worker_speed / avg_speed

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º: 0.5x - 2.0x
        speed_ratio = max(0.5, min(2.0, speed_ratio))

        adaptive_size = int(self.base_chunk_size * speed_ratio)

        # –û–∫—Ä—É–≥–ª—è–µ–º –¥–æ 100k
        adaptive_size = (adaptive_size // 100_000) * 100_000
        adaptive_size = max(100_000, adaptive_size)  # –ú–∏–Ω–∏–º—É–º 100k

        return adaptive_size


class DynamicChunkGenerator:
    """–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è chunk batches"""

    def __init__(
        self,
        charset: str,
        length: int,
        base_chunk_size: int = 1_000_000,
        lookahead_batches: int = 10
    ):
        self.charset = charset
        self.length = length
        self.base = len(charset)
        self.total_combinations = self.base ** length

        self.base_chunk_size = base_chunk_size
        self.lookahead_batches = lookahead_batches

        # –°–æ—Å—Ç–æ—è–Ω–∏–µ
        self.current_version = 0
        self.current_global_index = 0
        self.generated_batches: Dict[int, BatchInfo] = {}
        self.completed_batches = set()

        # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        self.performance = PerformanceAnalyzer(base_chunk_size)

        # Lock –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        self._generation_lock = asyncio.Lock()

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        self.logger = logging.getLogger("DynamicChunkGenerator")

    def index_to_combination(self, idx: int) -> str:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∏–Ω–¥–µ–∫—Å –≤ –∫–æ–º–±–∏–Ω–∞—Ü–∏—é —Å–∏–º–≤–æ–ª–æ–≤"""
        result = []
        for _ in range(self.length):
            result.append(self.charset[idx % self.base])
            idx //= self.base
        return ''.join(reversed(result))

    async def ensure_lookahead_batches(self, active_workers: List[str]):
        """
        –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –Ω–∞–ª–∏—á–∏–µ lookahead —á–∞–Ω–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–æ—Ä–∫–µ—Ä–∞

        –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã–µ (–Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ) —á–∞–Ω–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–æ—Ä–∫–µ—Ä–∞
        –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–µ –±–∞—Ç—á–∏, –ø–æ–∫–∞ —É —Å–∞–º–æ–≥–æ "–≥–æ–ª–æ–¥–Ω–æ–≥–æ" –≤–æ—Ä–∫–µ—Ä–∞
        –µ—Å—Ç—å –º–µ–Ω—å—à–µ lookahead_batches —á–∞–Ω–∫–æ–≤ –≤ –æ—á–µ—Ä–µ–¥–∏.
        """
        import logging
        logger = logging.getLogger("DynamicChunkGenerator")

        # –ù–µ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –±–∞—Ç—á–∏ –µ—Å–ª–∏ –Ω–µ—Ç –≤–æ—Ä–∫–µ—Ä–æ–≤
        if not active_workers:
            logger.debug("No active workers - skipping batch generation")
            return

        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–æ—Ä–∫–µ—Ä–∞ —Å—á–∏—Ç–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ (–Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ) —á–∞–Ω–∫–∏
        min_available_chunks = float('inf')
        worker_chunk_counts = {}

        for worker_id in active_workers:
            available_chunks = 0
            for batch in self.generated_batches.values():
                for chunk in batch.chunks:
                    if chunk.assigned_worker == worker_id and chunk.status in ("assigned", "working"):
                        available_chunks += 1

            worker_chunk_counts[worker_id] = available_chunks
            min_available_chunks = min(min_available_chunks, available_chunks)

        if min_available_chunks == float('inf'):
            min_available_chunks = 0

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –±–∞—Ç—á–∏ –ø–æ–∫–∞ —É —Å–∞–º–æ–≥–æ "–≥–æ–ª–æ–¥–Ω–æ–≥–æ" –≤–æ—Ä–∫–µ—Ä–∞ –º–µ–Ω—å—à–µ lookahead_batches —á–∞–Ω–∫–æ–≤
        needed = self.lookahead_batches - min_available_chunks

        logger.debug(
            f"Lookahead check: worker_chunks={worker_chunk_counts}, "
            f"min_available={min_available_chunks}, needed={needed}, "
            f"progress={self.current_global_index}/{self.total_combinations}"
        )

        if needed > 0 and self.current_global_index < self.total_combinations:
            logger.info(f"Generating {needed} new batches (min worker chunks: {min_available_chunks}/{self.lookahead_batches})")
            for _ in range(needed):
                if self.current_global_index >= self.total_combinations:
                    break
                batch = await self._generate_next_batch(active_workers)
                if batch is None:
                    logger.warning("Failed to generate batch despite having active workers")
                    break

    async def _generate_next_batch(self, active_workers: List[str]) -> Optional[BatchInfo]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–ª–µ–¥—É—é—â–∏–π batch"""
        if not active_workers:
            return None

        async with self._generation_lock:
            self.current_version += 1

            batch_chunks = []

            for worker_id in active_workers:
                if self.current_global_index >= self.total_combinations:
                    break

                # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ä–∞–∑–º–µ—Ä
                chunk_size = self.performance.calculate_adaptive_chunk_size(worker_id)

                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–º—Å—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ–º
                remaining = self.total_combinations - self.current_global_index
                chunk_size = min(chunk_size, remaining)

                chunk = ChunkInfo(
                    chunk_id=self.current_version * 10000 + len(batch_chunks),
                    start_index=self.current_global_index,
                    end_index=self.current_global_index + chunk_size,
                    chunk_size=chunk_size,
                    assigned_worker=worker_id,
                    status="assigned",
                    priority=1,
                    created_at=time.time()
                )

                batch_chunks.append(chunk)
                self.current_global_index += chunk_size

            if not batch_chunks:
                return None

            batch = BatchInfo(
                version=self.current_version,
                chunks=batch_chunks,
                created_at=time.time(),
                is_recovery=False
            )

            self.generated_batches[self.current_version] = batch

            return batch

    async def recover_orphaned_chunks(
        self,
        orphaned: List[dict],
        active_workers: List[str]
    ) -> Optional[BatchInfo]:
        """–°–æ–∑–¥–∞–µ—Ç recovery batch –∏–∑ orphaned chunks"""
        if not orphaned or not active_workers:
            return None

        async with self._generation_lock:
            self.current_version += 1

            recovery_chunks = []

            for i, orphan in enumerate(orphaned):
                # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                start_idx = orphan.get("progress", orphan["start_index"]) + 1
                end_idx = orphan["end_index"]

                if start_idx >= end_idx:
                    continue  # –£–∂–µ –∑–∞–≤–µ—Ä—à–µ–Ω

                chunk_size = end_idx - start_idx
                worker_id = active_workers[i % len(active_workers)]

                chunk = ChunkInfo(
                    chunk_id=self.current_version * 10000 + len(recovery_chunks),
                    start_index=start_idx,
                    end_index=end_idx,
                    chunk_size=chunk_size,
                    assigned_worker=worker_id,
                    status="recovery",
                    priority=5,
                    created_at=time.time()
                )

                recovery_chunks.append(chunk)

            if not recovery_chunks:
                return None

            batch = BatchInfo(
                version=self.current_version,
                chunks=recovery_chunks,
                created_at=time.time(),
                is_recovery=True
            )

            self.generated_batches[self.current_version] = batch

            return batch

    def mark_batch_completed(self, version: int):
        """–ü–æ–º–µ—á–∞–µ—Ç batch –∫–∞–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–π"""
        self.completed_batches.add(version)

        # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –±–∞—Ç—á–∏
        if len(self.completed_batches) > 20:
            old_versions = sorted(self.completed_batches)[:-20]
            for v in old_versions:
                if v in self.generated_batches:
                    del self.generated_batches[v]
                self.completed_batches.discard(v)

    def chunk_completed(self, chunk_id: int, hash_count: int, solutions: List[dict]):
        """
        –ü–æ–º–µ—á–∞–µ—Ç —á–∞–Ω–∫ –∫–∞–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–π

        Args:
            chunk_id: ID —á–∞–Ω–∫–∞
            hash_count: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã—Ö —Ö–µ—à–µ–π
            solutions: –ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è
        """
        # –ù–∞—Ö–æ–¥–∏–º —á–∞–Ω–∫ –≤ –±–∞—Ç—á–∞—Ö
        found = False
        batch_version = None

        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –õ–æ–≥–∏—Ä—É–µ–º –ø–æ–ø—ã—Ç–∫—É –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
        self.logger.info(f"üîç [DIAG] chunk_completed called for chunk_id={chunk_id}, hash_count={hash_count}, solutions={len(solutions)}")

        for batch in self.generated_batches.values():
            for chunk in batch.chunks:
                # –ü—Ä–∏–≤–æ–¥–∏–º –∫ int –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (–º–æ–∂–µ—Ç –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π –∏–∑ gossip)
                if int(chunk.chunk_id) == int(chunk_id):
                    old_status = chunk.status
                    chunk.status = "solved"
                    found = True
                    batch_version = batch.version

                    self.logger.info(f"‚úÖ [DIAG] Chunk {chunk_id} status: {old_status} ‚Üí solved (batch_version={batch_version})")

                    # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –≤ _process_worker_chunk_status
                    # –≥–¥–µ –µ—Å—Ç—å –¥–æ—Å—Ç—É–ø –∫ time_taken –∏–∑ gossip
                    break
            if found:
                break

        if not found:
            available_chunks = [(b.version, c.chunk_id, c.status) for b in self.generated_batches.values() for c in b.chunks]
            self.logger.warning(f"‚ùå [DIAG] Chunk {chunk_id} NOT FOUND in batches! Available: {available_chunks}")
            return

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤—Å–µ –ª–∏ —á–∞–Ω–∫–∏ –±–∞—Ç—á–∞ –∑–∞–≤–µ—Ä—à–µ–Ω—ã
        if batch_version is not None:
            batch = self.generated_batches[batch_version]
            all_solved = all(chunk.status == "solved" for chunk in batch.chunks)

            if all_solved:
                # –ü–æ–º–µ—á–∞–µ–º –±–∞—Ç—á –∫–∞–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–π
                self.mark_batch_completed(batch_version)
                self.logger.info(f"Batch {batch_version} completed (all {len(batch.chunks)} chunks solved)")

    def chunk_failed(self, chunk_id: int):
        """
        –ü–æ–º–µ—á–∞–µ—Ç —á–∞–Ω–∫ –∫–∞–∫ –ø—Ä–æ–≤–∞–ª–µ–Ω–Ω—ã–π (–¥–ª—è –ø–µ—Ä–µ–Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è)

        Args:
            chunk_id: ID —á–∞–Ω–∫–∞
        """
        # –ù–∞—Ö–æ–¥–∏–º —á–∞–Ω–∫ –∏ –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ recovery
        for batch in self.generated_batches.values():
            for chunk in batch.chunks:
                if chunk.chunk_id == chunk_id:
                    chunk.status = "timeout"  # –ë—É–¥–µ—Ç –ø–µ—Ä–µ–Ω–∞–∑–Ω–∞—á–µ–Ω –∫–∞–∫ recovery
                    return

    def get_progress(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"""
        processed = 0
        in_progress = 0

        for batch in self.generated_batches.values():
            for chunk in batch.chunks:
                if chunk.status == "solved":
                    processed += chunk.chunk_size
                elif chunk.status in ("working", "assigned"):
                    in_progress += chunk.chunk_size

        progress_pct = (processed / self.total_combinations * 100) if self.total_combinations > 0 else 0

        cluster_stats = self.performance.calculate_cluster_stats()
        total_hash_rate = cluster_stats["total_speed"]

        remaining = self.total_combinations - processed
        eta_seconds = (remaining / total_hash_rate) if total_hash_rate > 0 else 0

        return {
            "total_combinations": self.total_combinations,
            "processed": processed,
            "in_progress": in_progress,
            "pending": remaining,
            "progress_percentage": progress_pct,
            "eta_seconds": eta_seconds,
            "current_version": self.current_version,
            "completed_batches": len(self.completed_batches),
            "active_batches": len(self.generated_batches) - len(self.completed_batches)
        }


class Run(BaseService):
    SERVICE_NAME = "hash_coordinator"

    def __init__(self, service_name: str, proxy_client=None):
        super().__init__(service_name, proxy_client)
        self.info.version = "1.1.0"
        self.info.description = "–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π —Ö–µ—à–µ–π"

        # –ê–∫—Ç–∏–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏
        self.active_jobs: Dict[str, DynamicChunkGenerator] = {}

        # –ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏
        self.job_solutions: Dict[str, List[dict]] = {}  # {job_id: [solutions]}

        # –°–æ—Å—Ç–æ—è–Ω–∏–µ –≤–æ—Ä–∫–µ—Ä–æ–≤ –∏–∑ gossip
        self.worker_states: Dict[str, dict] = {}

        # Background tasks
        self.monitor_task = None
        self.orphaned_detection_task = None

    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞"""
        # –¢–æ–ª—å–∫–æ –Ω–∞ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–µ
        if not self.context.config.coordinator_mode:
            self.logger.info("Hash coordinator disabled on worker node")
            return

        self.logger.info("Hash coordinator initialized")

        # –ó–∞–ø—É—Å–∫–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
        self.monitor_task = asyncio.create_task(self._monitor_loop())
        self.orphaned_detection_task = asyncio.create_task(self._orphaned_detection_loop())

    async def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        if self.monitor_task:
            self.monitor_task.cancel()
        if self.orphaned_detection_task:
            self.orphaned_detection_task.cancel()

    @service_method(description="–°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ö–µ—à–µ–π", public=True)
    async def create_job(
        self,
        job_id: str,
        mode: str = "brute",  # "brute" or "dictionary"
        charset: Optional[str] = None,
        length: Optional[int] = None,
        wordlist: Optional[List[str]] = None,
        mutations: Optional[List[str]] = None,
        hash_algo: str = "sha256",
        target_hash: Optional[str] = None,
        target_hashes: Optional[List[str]] = None,  # Multi-target mode
        ssid: Optional[str] = None,  # For WPA/WPA2
        base_chunk_size: int = 1_000_000,
        lookahead_batches: int = 10  # Number of batches to generate ahead
    ) -> Dict[str, Any]:
        """
        –°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ö–µ—à–µ–π

        Args:
            job_id: –£–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –∑–∞–¥–∞—á–∏
            mode: –†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã ("brute" –∏–ª–∏ "dictionary")
            charset: –ù–∞–±–æ—Ä —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –ø–µ—Ä–µ–±–æ—Ä–∞ (–¥–ª—è brute mode)
            length: –î–ª–∏–Ω–∞ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ (–¥–ª—è brute mode)
            wordlist: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤ (–¥–ª—è dictionary mode)
            mutations: –ü—Ä–∞–≤–∏–ª–∞ –º—É—Ç–∞—Ü–∏–∏ (–¥–ª—è dictionary mode)
            hash_algo: –ê–ª–≥–æ—Ä–∏—Ç–º —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è
            target_hash: –¶–µ–ª–µ–≤–æ–π —Ö–µ—à (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            target_hashes: –°–ø–∏—Å–æ–∫ —Ü–µ–ª–µ–≤—ã—Ö —Ö–µ—à–µ–π (multi-target mode)
            ssid: SSID –¥–ª—è WPA/WPA2 cracking
            base_chunk_size: –ë–∞–∑–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
            lookahead_batches: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –Ω–∞–ø–µ—Ä–µ–¥ (default: 10)
        """
        if job_id in self.active_jobs:
            return {"success": False, "error": "Job already exists"}

        # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if mode == "brute":
            if not charset or not length:
                return {"success": False, "error": "charset and length required for brute mode"}

            # –°–æ–∑–¥–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–ª—è brute force
            generator = DynamicChunkGenerator(
                charset=charset,
                length=length,
                base_chunk_size=base_chunk_size,
                lookahead_batches=lookahead_batches
            )
            total_items = generator.total_combinations

        elif mode == "dictionary":
            if not wordlist:
                return {"success": False, "error": "wordlist required for dictionary mode"}

            # –î–ª—è dictionary mode –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –∫–∞–∫ total
            total_items = len(wordlist)

            # –°–æ–∑–¥–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å —Ñ–∏–∫—Ç–∏–≤–Ω—ã–º charset
            generator = DynamicChunkGenerator(
                charset="a",  # Dummy
                length=1,
                base_chunk_size=base_chunk_size,
                lookahead_batches=lookahead_batches
            )
            generator.total_combinations = total_items

        else:
            return {"success": False, "error": f"Unknown mode: {mode}"}

        self.active_jobs[job_id] = generator
        self.job_solutions[job_id] = []  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ —Ä–µ—à–µ–Ω–∏–π

        # –ü–æ–ª—É—á–∞–µ–º –∞–∫—Ç–∏–≤–Ω—ã—Ö –≤–æ—Ä–∫–µ—Ä–æ–≤ –∏–∑ gossip
        active_workers = await self._get_active_workers()

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–µ –±–∞—Ç—á–∏
        await generator.ensure_lookahead_batches(active_workers)

        # –ü—É–±–ª–∏–∫—É–µ–º job metadata –≤ gossip
        await self._publish_job_metadata_v2(
            job_id=job_id,
            mode=mode,
            charset=charset,
            length=length,
            wordlist=wordlist,
            mutations=mutations,
            hash_algo=hash_algo,
            target_hash=target_hash,
            target_hashes=target_hashes,
            ssid=ssid
        )

        # –ü—É–±–ª–∏–∫—É–µ–º –ø–µ—Ä–≤—ã–µ –±–∞—Ç—á–∏
        await self._publish_batches(job_id, generator)

        return {
            "success": True,
            "job_id": job_id,
            "mode": mode,
            "total_combinations": total_items,
            "initial_batches": generator.current_version
        }

    @service_method(description="–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏", public=True)
    async def get_job_status(self, job_id: str) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏"""
        if job_id not in self.active_jobs:
            return {"success": False, "error": "Job not found"}

        generator = self.active_jobs[job_id]
        progress = generator.get_progress()
        cluster_stats = generator.performance.calculate_cluster_stats()
        solutions = self.job_solutions.get(job_id, [])

        return {
            "success": True,
            "job_id": job_id,
            "progress": progress,
            "cluster_stats": cluster_stats,
            "worker_speeds": generator.performance.worker_speeds,
            "solutions": solutions,
            "solutions_count": len(solutions)
        }

    @service_method(description="–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∑–∞–¥–∞—á", public=True)
    async def get_all_jobs(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á"""
        jobs = []

        for job_id, generator in self.active_jobs.items():
            progress = generator.get_progress()
            jobs.append({
                "job_id": job_id,
                "progress_percentage": progress["progress_percentage"],
                "processed": progress["processed"],
                "total": progress["total_combinations"],
                "eta_seconds": progress["eta_seconds"]
            })

        return {"success": True, "jobs": jobs}

    @service_method(description="–û—Ç—á–µ—Ç –æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏—è—Ö –æ—Ç –≤–æ—Ä–∫–µ—Ä–∞", public=True)
    async def report_solution(
        self,
        job_id: str,
        chunk_id: int,
        worker_id: str,
        solutions: List[dict]
    ) -> Dict[str, Any]:
        """
        –ü—Ä–∏–Ω–∏–º–∞–µ—Ç —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏—è—Ö –æ—Ç –≤–æ—Ä–∫–µ—Ä–∞

        Args:
            job_id: ID –∑–∞–¥–∞—á–∏
            chunk_id: ID —á–∞–Ω–∫–∞
            worker_id: ID –≤–æ—Ä–∫–µ—Ä–∞
            solutions: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π

        Returns:
            –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –ø–æ–ª—É—á–µ–Ω–∏—è
        """
        if job_id not in self.active_jobs:
            return {"success": False, "error": f"Job {job_id} not found"}

        self.logger.warning(f"Worker {worker_id} found {len(solutions)} solutions in job {job_id}, chunk {chunk_id}!")
        for sol in solutions:
            self.logger.warning(f"  Solution: {sol.get('combination')} ‚Üí {sol.get('hash')}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ—à–µ–Ω–∏—è
        if job_id not in self.job_solutions:
            self.job_solutions[job_id] = []

        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Ä–µ—à–µ–Ω–∏—è (–∏–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ hash)
        existing_hashes = {sol.get('hash') for sol in self.job_solutions[job_id]}
        for sol in solutions:
            if sol.get('hash') not in existing_hashes:
                self.job_solutions[job_id].append(sol)
                existing_hashes.add(sol.get('hash'))

        return {
            "success": True,
            "job_id": job_id,
            "solutions_count": len(solutions),
            "total_solutions": len(self.job_solutions[job_id]),
            "acknowledged": True
        }

    @service_method(description="–ò–º–ø–æ—Ä—Ç WPA handshake –∏–∑ PCAP", public=True)
    async def import_pcap(
        self,
        pcap_file: str,
        job_id_prefix: str = "wpa"
    ) -> Dict[str, Any]:
        """
        –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç WiFi handshakes –∏–∑ PCAP —Ñ–∞–π–ª–∞ –∏ —Å–æ–∑–¥–∞–µ—Ç –∑–∞–¥–∞—á–∏

        Args:
            pcap_file: –ü—É—Ç—å –∫ PCAP —Ñ–∞–π–ª—É
            job_id_prefix: –ü—Ä–µ—Ñ–∏–∫—Å –¥–ª—è job_id

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á
        """
        if PCAPParser is None:
            return {"success": False, "error": "PCAP parser not available"}

        try:
            parser = PCAPParser(pcap_file)
            handshakes = parser.parse()

            if not handshakes:
                return {"success": False, "error": "No handshakes found in PCAP"}

            created_jobs = []

            for i, hs in enumerate(handshakes):
                job_id = f"{job_id_prefix}_{hs['bssid'].replace(':', '')}_{i}"
                essid = hs['essid'] or f"unknown_{i}"

                # –°–æ–∑–¥–∞–µ–º job –¥–ª—è WPA cracking
                # Note: —Ç—Ä–µ–±—É–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä—å –¥–ª—è dictionary attack
                # –∏–ª–∏ charset –¥–ª—è brute force

                created_jobs.append({
                    "job_id": job_id,
                    "bssid": hs['bssid'],
                    "essid": essid,
                    "eapol_frames": len(hs['eapol_frames'])
                })

            return {
                "success": True,
                "handshakes_found": len(handshakes),
                "jobs": created_jobs
            }

        except Exception as e:
            self.logger.error(f"Failed to import PCAP: {e}")
            return {"success": False, "error": str(e)}

    @service_method(description="–≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ JSON/CSV", public=True)
    async def export_results(
        self,
        job_id: str,
        format: str = "json",  # "json" or "csv"
        output_file: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–∞–¥–∞—á–∏ –≤ —Ñ–∞–π–ª

        Args:
            job_id: ID –∑–∞–¥–∞—á–∏
            format: –§–æ—Ä–º–∞—Ç —ç–∫—Å–ø–æ—Ä—Ç–∞ ("json" –∏–ª–∏ "csv")
            output_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É (–µ—Å–ª–∏ None, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ)

        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–ª–∏ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
        """
        if job_id not in self.active_jobs:
            return {"success": False, "error": "Job not found"}

        generator = self.active_jobs[job_id]
        progress = generator.get_progress()

        # –ü–æ–ª—É—á–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è
        solutions = self.job_solutions.get(job_id, [])

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results = {
            "job_id": job_id,
            "progress": progress,
            "solutions": solutions,
            "solutions_count": len(solutions),
            "exported_at": datetime.now().isoformat()
        }

        if output_file is None:
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞–ø—Ä—è–º—É—é
            return {
                "success": True,
                "format": format,
                "data": results
            }

        try:
            # –≠–∫—Å–ø–æ—Ä—Ç –≤ —Ñ–∞–π–ª
            if format == "json":
                with open(output_file, 'w') as f:
                    json.dump(results, f, indent=2)

            elif format == "csv":
                with open(output_file, 'w', newline='') as f:
                    writer = csv.DictWriter(f, fieldnames=['combination', 'hash', 'index'])
                    writer.writeheader()
                    for sol in solutions:
                        writer.writerow(sol)

            else:
                return {"success": False, "error": f"Unknown format: {format}"}

            return {
                "success": True,
                "format": format,
                "output_file": output_file,
                "solutions_count": len(solutions)
            }

        except Exception as e:
            self.logger.error(f"Failed to export results: {e}")
            return {"success": False, "error": str(e)}

    @service_method(description="–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—Ç –≤–æ—Ä–∫–µ—Ä–∞", public=True)
    async def report_chunk_progress(
        self,
        job_id: str,
        worker_id: str,
        chunk_id: int,
        status: str,
        progress: Optional[int] = None,
        time_taken: Optional[float] = None,
        solutions: Optional[List[dict]] = None
    ) -> Dict[str, Any]:
        """
        –í–æ—Ä–∫–µ—Ä –æ—Ç—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ

        –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –í –æ—Å–Ω–æ–≤–Ω–æ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è gossip, –Ω–æ —ç—Ç–æ—Ç –º–µ—Ç–æ–¥
        –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–µ–Ω –¥–ª—è —è–≤–Ω—ã—Ö –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π
        """
        if job_id not in self.active_jobs:
            return {"success": False, "error": "Job not found"}

        generator = self.active_jobs[job_id]

        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        if status == "solved" and time_taken:
            for batch in generator.generated_batches.values():
                for chunk in batch.chunks:
                    if chunk.chunk_id == chunk_id:
                        generator.performance.update_worker_performance(
                            worker_id,
                            chunk.chunk_size,
                            time_taken
                        )
                        chunk.status = "solved"
                        break

        return {"success": True}

    async def _get_active_workers(self) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∞–∫—Ç–∏–≤–Ω—ã—Ö –≤–æ—Ä–∫–µ—Ä–æ–≤ –∏–∑ gossip"""
        network = self.context.get_shared("network")
        if not network:
            return []

        nodes = network.gossip.node_registry

        # –§–∏–ª—å—Ç—Ä—É–µ–º –≤–æ—Ä–∫–µ—Ä–æ–≤ —Å —Å–µ—Ä–≤–∏—Å–æ–º hash_worker
        workers = []
        for node_id, node_info in nodes.items():
            if node_info.role == "worker":
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ hash_worker –≤ —Å–µ—Ä–≤–∏—Å–∞—Ö
                if "hash_worker" in node_info.services:
                    workers.append(node_id)

        return workers

    async def _publish_job_metadata(
        self,
        job_id: str,
        charset: str,
        length: int,
        hash_algo: str,
        target_hash: Optional[str]
    ):
        """–ü—É–±–ª–∏–∫—É–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ –≤ gossip (legacy)"""
        await self._publish_job_metadata_v2(
            job_id=job_id,
            mode="brute",
            charset=charset,
            length=length,
            hash_algo=hash_algo,
            target_hash=target_hash
        )

    async def _publish_job_metadata_v2(
        self,
        job_id: str,
        mode: str,
        charset: Optional[str] = None,
        length: Optional[int] = None,
        wordlist: Optional[List[str]] = None,
        mutations: Optional[List[str]] = None,
        hash_algo: str = "sha256",
        target_hash: Optional[str] = None,
        target_hashes: Optional[List[str]] = None,
        ssid: Optional[str] = None
    ):
        """–ü—É–±–ª–∏–∫—É–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ –≤ gossip (v2 —Å –Ω–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏)"""
        network = self.context.get_shared("network")
        if not network:
            return

        job_metadata = {
            "job_id": job_id,
            "mode": mode,
            "charset": charset,
            "length": length,
            "wordlist": wordlist,
            "mutations": mutations,
            "hash_algo": hash_algo,
            "target_hash": target_hash,
            "target_hashes": target_hashes,
            "ssid": ssid,
            "started_at": time.time()
        }

        # Use new versioned update_metadata API
        network.gossip.update_metadata(f"hash_job_{job_id}", job_metadata)

    def _merge_batch_statuses(self, current_batches: dict, new_batches: dict) -> dict:
        """
        –ú–µ—Ä–∂–∏—Ç —Å—Ç–∞—Ç—É—Å—ã —á–∞–Ω–∫–æ–≤ –≤ batches —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º solved > working > recovery > assigned

        Args:
            current_batches: –¢–µ–∫—É—â–∏–µ batches –∏–∑ gossip
            new_batches: –ù–æ–≤—ã–µ batches –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏

        Returns:
            –ó–∞–º–µ—Ä–∂–µ–Ω–Ω—ã–µ batches
        """
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã —Å—Ç–∞—Ç—É—Å–æ–≤ (–≤—ã—à–µ = –≤–∞–∂–Ω–µ–µ)
        status_priority = {
            "solved": 4,
            "working": 3,
            "recovery": 2,
            "timeout": 1,
            "assigned": 0
        }

        merged = {}

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –≤–µ—Ä—Å–∏–∏ –∏–∑ –æ–±–æ–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        all_versions = set(current_batches.keys()) | set(new_batches.keys())

        for version in all_versions:
            current_batch = current_batches.get(version, {})
            new_batch = new_batches.get(version, {})

            # –ï—Å–ª–∏ –≤–µ—Ä—Å–∏—è —Ç–æ–ª—å–∫–æ –≤ –æ–¥–Ω–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–µ - –±–µ—Ä–µ–º –∫–∞–∫ –µ—Å—Ç—å
            if not current_batch:
                merged[version] = new_batch
                continue
            if not new_batch:
                merged[version] = current_batch
                continue

            # –ú–µ—Ä–∂–∏–º chunks
            current_chunks = current_batch.get("chunks", {})
            new_chunks = new_batch.get("chunks", {})

            merged_chunks = {}
            all_chunk_ids = set(current_chunks.keys()) | set(new_chunks.keys())

            for chunk_id in all_chunk_ids:
                current_chunk = current_chunks.get(chunk_id)
                new_chunk = new_chunks.get(chunk_id)

                # –ï—Å–ª–∏ chunk —Ç–æ–ª—å–∫–æ –≤ –æ–¥–Ω–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–µ
                if not current_chunk:
                    merged_chunks[chunk_id] = new_chunk
                    continue
                if not new_chunk:
                    merged_chunks[chunk_id] = current_chunk
                    continue

                # –û–±–∞ chunk –µ—Å—Ç—å - –≤—ã–±–∏—Ä–∞–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É —Å—Ç–∞—Ç—É—Å–∞
                current_status = current_chunk.get("status", "assigned")
                new_status = new_chunk.get("status", "assigned")

                current_priority = status_priority.get(current_status, 0)
                new_priority = status_priority.get(new_status, 0)

                if new_priority >= current_priority:
                    merged_chunks[chunk_id] = new_chunk
                else:
                    merged_chunks[chunk_id] = current_chunk

            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–º–µ—Ä–∂–µ–Ω–Ω—ã–π batch
            merged[version] = {
                "chunks": merged_chunks,
                "created_at": new_batch.get("created_at", current_batch.get("created_at")),
                "is_recovery": new_batch.get("is_recovery", current_batch.get("is_recovery", False))
            }

        return merged

    async def _publish_batches(self, job_id: str, generator: DynamicChunkGenerator):
        """–ü—É–±–ª–∏–∫—É–µ—Ç batches –≤ gossip —Å –º–µ—Ä–∂–µ–º —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        network = self.context.get_shared("network")
        if not network:
            return

        # –ü—É–±–ª–∏–∫—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ –±–∞—Ç—á–∏
        new_batches = {}

        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –°—á–µ—Ç—á–∏–∫–∏ —Å—Ç–∞—Ç—É—Å–æ–≤ –î–û –º–µ—Ä–∂–∞
        new_status_counts = {"assigned": 0, "working": 0, "solved": 0, "recovery": 0, "timeout": 0}

        for version, batch in generator.generated_batches.items():
            if version not in generator.completed_batches:
                # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É: {chunk_id: {assigned_worker: data}}
                chunks_dict = {}
                for chunk in batch.chunks:
                    chunks_dict[chunk.chunk_id] = {
                        "assigned_worker": chunk.assigned_worker,
                        "start_index": chunk.start_index,
                        "end_index": chunk.end_index,
                        "chunk_size": chunk.chunk_size,
                        "status": chunk.status,
                        "priority": chunk.priority
                    }

                    # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –°—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ç—É—Å—ã
                    new_status_counts[chunk.status] = new_status_counts.get(chunk.status, 0) + 1

                new_batches[version] = {
                    "chunks": chunks_dict,
                    "created_at": batch.created_at,
                    "is_recovery": batch.is_recovery
                }

        # –ß–∏—Ç–∞–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏–∑ gossip
        batches_key = f"hash_batches_{job_id}"
        current_batches = network.gossip.node_registry[network.gossip.node_id].metadata.get(batches_key, {})

        # –ú–ï–†–ñ–ò–ú —Å —Ç–µ–∫—É—â–∏–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º (solved –∏–º–µ–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç!)
        merged_batches = self._merge_batch_statuses(current_batches, new_batches)

        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –°—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ç—É—Å—ã –ü–û–°–õ–ï –º–µ—Ä–∂–∞
        merged_status_counts = {"assigned": 0, "working": 0, "solved": 0, "recovery": 0, "timeout": 0}
        for batch_data in merged_batches.values():
            for chunk_data in batch_data.get("chunks", {}).values():
                status = chunk_data.get("status", "assigned")
                merged_status_counts[status] = merged_status_counts.get(status, 0) + 1

        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –õ–æ–≥–∏—Ä—É–µ–º –º–µ—Ä–∂
        if new_status_counts != merged_status_counts:
            self.logger.info(
                f"üîÄ [DIAG] Merged batches for {job_id}: "
                f"new={new_status_counts} + current={len(current_batches)} versions ‚Üí "
                f"merged={merged_status_counts}"
            )
        else:
            self.logger.info(f"üì§ [DIAG] Publishing batches for {job_id}: {len(merged_batches)} batches, statuses: {merged_status_counts}")

        # Use new versioned update_metadata API
        network.gossip.update_metadata(batches_key, merged_batches)

    async def _monitor_loop(self):
        """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∑–∞–¥–∞—á"""
        while True:
            try:
                await asyncio.sleep(10)  # –ö–∞–∂–¥—ã–µ 10 —Å–µ–∫—É–Ω–¥

                # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –≤–æ—Ä–∫–µ—Ä–æ–≤ –∏–∑ gossip
                await self._update_worker_states()

                # –î–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏
                for job_id, generator in self.active_jobs.items():
                    # –ü–æ–ª—É—á–∞–µ–º –∞–∫—Ç–∏–≤–Ω—ã—Ö –≤–æ—Ä–∫–µ—Ä–æ–≤
                    active_workers = await self._get_active_workers()

                    # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º lookahead –±–∞—Ç—á–∏
                    await generator.ensure_lookahead_batches(active_workers)

                    # –ü—É–±–ª–∏–∫—É–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –±–∞—Ç—á–∏
                    await self._publish_batches(job_id, generator)

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
                    if generator.current_global_index >= generator.total_combinations:
                        progress = generator.get_progress()
                        if progress["pending"] == 0 and progress["in_progress"] == 0:
                            self.logger.info(f"Job {job_id} completed!")
                            # TODO: –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error(f"Error in monitor loop: {e}")

    async def _orphaned_detection_loop(self):
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ orphaned chunks"""
        while True:
            try:
                await asyncio.sleep(60)  # –ö–∞–∂–¥—É—é –º–∏–Ω—É—Ç—É

                for job_id, generator in self.active_jobs.items():
                    orphaned = await self._detect_orphaned_chunks(generator)

                    if orphaned:
                        self.logger.warning(f"Detected {len(orphaned)} orphaned chunks in {job_id}")

                        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º
                        active_workers = await self._get_active_workers()
                        recovery_batch = await generator.recover_orphaned_chunks(
                            orphaned,
                            active_workers
                        )

                        if recovery_batch:
                            await self._publish_batches(job_id, generator)

            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error(f"Error in orphaned detection: {e}")

    async def _detect_orphaned_chunks(self, generator: DynamicChunkGenerator) -> List[dict]:
        """–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç orphaned chunks"""
        orphaned = []
        timeout_threshold = 300  # 5 –º–∏–Ω—É—Ç
        now = time.time()

        for batch in generator.generated_batches.values():
            for chunk in batch.chunks:
                if chunk.status == "working":
                    age = now - chunk.created_at

                    if age > timeout_threshold:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º: –µ—Å—Ç—å –ª–∏ –±–æ–ª–µ–µ –Ω–æ–≤—ã–µ —Ä–µ—à–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏?
                        has_newer_solved = any(
                            c.chunk_id > chunk.chunk_id and c.status == "solved"
                            for b in generator.generated_batches.values()
                            for c in b.chunks
                            if c.assigned_worker == chunk.assigned_worker
                        )

                        if has_newer_solved:
                            orphaned.append({
                                "chunk_id": chunk.chunk_id,
                                "start_index": chunk.start_index,
                                "end_index": chunk.end_index,
                                "progress": chunk.start_index,  # –ë–µ–∑ –∏–Ω—Ñ–æ –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ
                                "stuck_worker": chunk.assigned_worker,
                                "age": age
                            })

        return orphaned

    async def _update_worker_states(self):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏—è –≤–æ—Ä–∫–µ—Ä–æ–≤ –∏–∑ gossip –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏"""
        network = self.context.get_shared("network")
        if not network:
            return

        nodes = network.gossip.node_registry

        for node_id, node_info in nodes.items():
            if node_id in self.worker_states:
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ
                self.worker_states[node_id].update({
                    "last_seen": node_info.last_seen,
                    "status": node_info.status
                })
            else:
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ
                self.worker_states[node_id] = {
                    "node_id": node_id,
                    "last_seen": node_info.last_seen,
                    "status": node_info.status
                }

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º hash_worker_status –∏–∑ metadata
            worker_status = node_info.metadata.get("hash_worker_status")
            if worker_status and isinstance(worker_status, dict):
                await self._process_worker_chunk_status(node_id, worker_status)

    async def _process_worker_chunk_status(self, worker_id: str, status: dict):
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç—É—Å —á–∞–Ω–∫–∞ –æ—Ç –≤–æ—Ä–∫–µ—Ä–∞ –∏–∑ gossip

        Args:
            worker_id: ID –≤–æ—Ä–∫–µ—Ä–∞
            status: –î–∞–Ω–Ω—ã–µ –∏–∑ hash_worker_status
        """
        job_id = status.get("job_id")
        chunk_id = status.get("chunk_id")
        chunk_status = status.get("status")

        if not job_id or chunk_id is None:
            return

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∑–∞–¥–∞—á–∞ –∞–∫—Ç–∏–≤–Ω–∞
        if job_id not in self.active_jobs:
            return

        generator = self.active_jobs[job_id]

        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –õ–æ–≥–∏—Ä—É–µ–º –≤—Å–µ —Å—Ç–∞—Ç—É—Å—ã –æ—Ç –≤–æ—Ä–∫–µ—Ä–æ–≤
        self.logger.info(f"üîç [DIAG] Worker {worker_id} reported chunk {chunk_id} status: {chunk_status}")

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç—É—Å "solved" - —á–∞–Ω–∫ –∑–∞–≤–µ—Ä—à–µ–Ω
        if chunk_status == "solved":
            hash_count = status.get("hash_count", 0)
            time_taken = status.get("time_taken", 0)
            solutions = status.get("solutions", [])

            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å —á–∞–Ω–∫–∞ –≤ generator
            generator.chunk_completed(chunk_id, hash_count, solutions)

            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤–æ—Ä–∫–µ—Ä–∞
            if time_taken > 0:
                # –ù–∞—Ö–æ–¥–∏–º chunk_size –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                chunk_size = 0
                for batch in generator.generated_batches.values():
                    for chunk in batch.chunks:
                        if int(chunk.chunk_id) == int(chunk_id):
                            chunk_size = chunk.chunk_size
                            break
                    if chunk_size > 0:
                        break

                if chunk_size > 0:
                    generator.performance.update_worker_performance(
                        worker_id,
                        chunk_size,
                        time_taken
                    )

            if solutions:
                self.logger.warning(f"Worker {worker_id} found {len(solutions)} solutions in chunk {chunk_id}!")
                for sol in solutions:
                    self.logger.warning(f"  Solution: {sol.get('combination')} ‚Üí {sol.get('hash')}")

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ—à–µ–Ω–∏—è
                if job_id not in self.job_solutions:
                    self.job_solutions[job_id] = []

                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Ä–µ—à–µ–Ω–∏—è (–∏–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ hash)
                existing_hashes = {sol.get('hash') for sol in self.job_solutions[job_id]}
                for sol in solutions:
                    if sol.get('hash') not in existing_hashes:
                        self.job_solutions[job_id].append(sol)
                        existing_hashes.add(sol.get('hash'))

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ –±–∞—Ç—á–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ (lookahead)
            active_workers = await self._get_active_workers()
            await generator.ensure_lookahead_batches(active_workers)

            # –í–ê–ñ–ù–û: –ü—É–±–ª–∏–∫—É–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ batches –≤ gossip
            await self._publish_batches(job_id, generator)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç—É—Å "working" - –æ–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
        elif chunk_status == "working":
            progress = status.get("progress")
            if progress is not None:
                # –ú–æ–∂–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å —á–∞–Ω–∫–∞, –Ω–æ –ø–æ–∫–∞ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ
                pass
