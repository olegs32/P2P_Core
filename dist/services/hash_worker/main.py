"""
Hash Worker Service - –í–æ—Ä–∫–µ—Ä –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π —Ö–µ—à–µ–π

–§—É–Ω–∫—Ü–∏–∏:
- –ü–æ–ª—É—á–µ–Ω–∏–µ chunk batches —á–µ—Ä–µ–∑ gossip
- –í—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ö–µ—à–µ–π
- –û—Ç—á–µ—Ç–Ω–æ—Å—Ç—å –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ —á–µ—Ä–µ–∑ gossip
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π claim —á–∞–Ω–∫–æ–≤ –±–µ–∑ –∫–æ–ª–ª–∏–∑–∏–π
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ hash –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ (SHA-2/3, NTLM, WPA2, etc.)
- Dictionary attack —Å –º—É—Ç–∞—Ü–∏—è–º–∏
- Multi-target mode
"""

import asyncio
import hashlib
import logging
import time
import struct
import multiprocessing as mp
import psutil
import importlib.util
import sys
from pathlib import Path
from typing import Dict, List, Optional, Any, Set

from layers.service import BaseService, service_method

# Import worker functions from separate module (required for multiprocessing pickling)
# Use importlib to dynamically load module from same directory as this file
_current_dir = Path(__file__).parent
_worker_module_path = _current_dir / "hash_computer_workers.py"
_worker_module_name = "hash_computer_workers"

# Add service directory to sys.path if not already there
if str(_current_dir) not in sys.path:
    sys.path.insert(0, str(_current_dir))

# Load the worker module
_spec = importlib.util.spec_from_file_location(_worker_module_name, _worker_module_path)
_worker_module = importlib.util.module_from_spec(_spec)
sys.modules[_worker_module_name] = _worker_module
_spec.loader.exec_module(_worker_module)

# Import from loaded module
compute_brute_subchunk = _worker_module.compute_brute_subchunk
compute_dict_subchunk = _worker_module.compute_dict_subchunk
HashAlgorithms = _worker_module.HashAlgorithms
MutationEngine = _worker_module.MutationEngine


class SystemMonitor:
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã"""

    def __init__(self, max_cpu_percent: float = 80.0, max_memory_percent: float = 80.0):
        self.max_cpu_percent = max_cpu_percent
        self.max_memory_percent = max_memory_percent
        self.process = psutil.Process()

    def get_current_load(self) -> Dict[str, float]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â—É—é –∑–∞–≥—Ä—É–∑–∫—É —Å–∏—Å—Ç–µ–º—ã"""
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory_percent = psutil.virtual_memory().percent

        return {
            "cpu_percent": cpu_percent,
            "memory_percent": memory_percent,
            "process_cpu": self.process.cpu_percent(),
            "process_memory": self.process.memory_percent()
        }

    def is_overloaded(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω–∞ –ª–∏ —Å–∏—Å—Ç–µ–º–∞"""
        load = self.get_current_load()
        return (load["cpu_percent"] > self.max_cpu_percent or
                load["memory_percent"] > self.max_memory_percent)

    def calculate_optimal_workers(self, max_workers: int = None) -> int:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ worker –ø—Ä–æ—Ü–µ—Å—Å–æ–≤

        –§–æ—Ä–º—É–ª–∞: workers = cpu_count * (1 - current_load/100) * safety_factor
        """
        if max_workers is None:
            max_workers = mp.cpu_count()

        load = self.get_current_load()
        cpu_load = load["cpu_percent"] / 100.0

        # –î–æ—Å—Ç—É–ø–Ω–∞—è –º–æ—â–Ω–æ—Å—Ç—å = 1 - —Ç–µ–∫—É—â–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
        available_capacity = max(0.1, 1.0 - cpu_load)

        # Safety factor –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è overload
        safety_factor = 0.8

        optimal = int(max_workers * available_capacity * safety_factor)

        # –ú–∏–Ω–∏–º—É–º 1 worker, –º–∞–∫—Å–∏–º—É–º max_workers
        return max(1, min(optimal, max_workers))


class HashComputer:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ö–µ—à–µ–π —Å multiprocessing"""

    def __init__(
        self,
        charset: str = None,
        length: int = None,
        hash_algo: str = "sha256",
        mode: str = "brute",  # "brute" or "dictionary"
        wordlist: List[str] = None,
        mutations: List[str] = None,
        ssid: str = None,  # –î–ª—è WPA/WPA2
        use_multiprocessing: bool = True,
        max_workers: int = None,
        max_cpu_percent: float = 80.0,
        max_memory_percent: float = 80.0
    ):
        self.mode = mode
        self.hash_algo = hash_algo
        self.ssid = ssid
        self.use_multiprocessing = use_multiprocessing
        self.max_workers = max_workers or mp.cpu_count()

        # System monitor –¥–ª—è dynamic load balancing
        self.system_monitor = SystemMonitor(max_cpu_percent, max_memory_percent)

        # Pool –¥–ª—è multiprocessing (—Å–æ–∑–¥–∞–µ—Ç—Å—è lazy)
        self.pool = None

        # –î–ª—è brute force
        if mode == "brute":
            self.charset = charset
            self.charset_list = list(charset) if charset else []
            self.length = length
            self.base = len(charset) if charset else 0
            self.powers_of_base = [self.base ** i for i in range(length)] if charset and length else []

        # –î–ª—è dictionary
        elif mode == "dictionary":
            self.wordlist = wordlist or []
            self.mutations = mutations or []
            self.mutation_engine = MutationEngine()

    def _get_or_create_pool(self) -> mp.Pool:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç Pool —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º workers"""
        if not self.use_multiprocessing:
            return None

        # –í—ã—á–∏—Å–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ workers –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–π –∑–∞–≥—Ä—É–∑–∫–∏
        optimal_workers = self.system_monitor.calculate_optimal_workers(self.max_workers)

        # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º pool –µ—Å–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ workers –∏–∑–º–µ–Ω–∏–ª–æ—Å—å
        if self.pool is None or self.pool._processes != optimal_workers:
            if self.pool is not None:
                self.pool.close()
                self.pool.join()

            self.pool = mp.Pool(processes=optimal_workers)

        return self.pool

    def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ multiprocessing"""
        if self.pool is not None:
            self.pool.close()
            self.pool.join()
            self.pool = None

    def index_to_combination(self, idx: int) -> str:
        """
        –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å ‚Üí –∫–æ–º–±–∏–Ω–∞—Ü–∏—è
        –ú–∏–Ω–∏–º—É–º –æ–ø–µ—Ä–∞—Ü–∏–π, –º–∞–∫—Å–∏–º—É–º —Å–∫–æ—Ä–æ—Å—Ç—å
        """
        if self.mode != "brute":
            raise ValueError("index_to_combination only for brute mode")

        result = [None] * self.length

        for pos in range(self.length - 1, -1, -1):
            result[pos] = self.charset_list[idx % self.base]
            idx //= self.base

        return ''.join(result)

    def compute_chunk(
        self,
        start_index: int,
        end_index: int,
        target_hash: Optional[str] = None,
        target_hashes: Optional[List[str]] = None,  # Multi-target mode
        progress_callback=None,
        progress_interval: int = 10000
    ) -> Dict[str, Any]:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —Ö–µ—à–∏ –¥–ª—è –¥–∏–∞–ø–∞–∑–æ–Ω–∞ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é

        Args:
            start_index: –ù–∞—á–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å
            end_index: –ö–æ–Ω–µ—á–Ω—ã–π –∏–Ω–¥–µ–∫—Å
            target_hash: –¶–µ–ª–µ–≤–æ–π —Ö–µ—à (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            target_hashes: –°–ø–∏—Å–æ–∫ —Ü–µ–ª–µ–≤—ã—Ö —Ö–µ—à–µ–π –¥–ª—è multi-target mode
            progress_callback: –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç—á–µ—Ç–∞ –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ
            progress_interval: –ò–Ω—Ç–µ—Ä–≤–∞–ª –æ—Ç—á–µ—Ç–æ–≤ (–∏—Ç–µ—Ä–∞—Ü–∏–π)

        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        """
        if self.mode == "brute":
            return self._compute_brute_force(
                start_index, end_index, target_hash, target_hashes,
                progress_callback, progress_interval
            )
        elif self.mode == "dictionary":
            return self._compute_dictionary(
                start_index, end_index, target_hash, target_hashes,
                progress_callback, progress_interval
            )
        else:
            raise ValueError(f"Unknown mode: {self.mode}")

    def _compute_brute_force(
        self,
        start_index: int,
        end_index: int,
        target_hash: Optional[str],
        target_hashes: Optional[List[str]],
        progress_callback,
        progress_interval: int
    ) -> Dict[str, Any]:
        """Brute force —Ä–µ–∂–∏–º —Å multiprocessing"""
        start_time = time.time()
        total_hash_count = 0
        all_solutions = []

        # Multi-target mode - –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ hex –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ worker
        if target_hashes:
            target_hash_set_hex = target_hashes
        elif target_hash:
            target_hash_set_hex = [target_hash]
        else:
            target_hash_set_hex = None

        # –ï—Å–ª–∏ multiprocessing –æ—Ç–∫–ª—é—á–µ–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π single-threaded –∫–æ–¥
        if not self.use_multiprocessing:
            return self._compute_brute_force_single(
                start_index, end_index, target_hash, target_hashes,
                progress_callback, progress_interval
            )

        # –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º pool —Å optimal –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º workers
        pool = self._get_or_create_pool()
        if pool is None:
            return self._compute_brute_force_single(
                start_index, end_index, target_hash, target_hashes,
                progress_callback, progress_interval
            )

        # –†–∞–∑–±–∏–≤–∞–µ–º chunk –Ω–∞ sub-chunks –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        num_workers = pool._processes
        chunk_size = end_index - start_index
        subchunk_size = max(progress_interval, chunk_size // num_workers)

        tasks = []
        current = start_index

        while current < end_index:
            subchunk_end = min(current + subchunk_size, end_index)

            task_args = (
                current,
                subchunk_end,
                self.charset_list,
                self.length,
                self.hash_algo,
                self.ssid,
                target_hash_set_hex
            )

            tasks.append(task_args)
            current = subchunk_end

        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å async —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        results = pool.map(compute_brute_subchunk, tasks)

        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for solutions, hash_count in results:
            all_solutions.extend(solutions)
            total_hash_count += hash_count

            # –û—Ç—á–µ—Ç –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ
            if progress_callback:
                progress_callback(start_index + total_hash_count, total_hash_count)

        time_taken = time.time() - start_time
        hash_rate = total_hash_count / time_taken if time_taken > 0 else 0

        return {
            "hash_count": total_hash_count,
            "time_taken": time_taken,
            "hash_rate": hash_rate,
            "solutions": all_solutions,
            "start_index": start_index,
            "end_index": end_index,
            "mode": "brute",
            "workers_used": num_workers
        }

    def _compute_brute_force_single(
        self,
        start_index: int,
        end_index: int,
        target_hash: Optional[str],
        target_hashes: Optional[List[str]],
        progress_callback,
        progress_interval: int
    ) -> Dict[str, Any]:
        """Brute force single-threaded (fallback)"""
        start_time = time.time()
        hash_count = 0
        solutions = []

        # Multi-target mode
        if target_hashes:
            target_hash_set = {bytes.fromhex(h) for h in target_hashes}
        elif target_hash:
            target_hash_set = {bytes.fromhex(target_hash)}
        else:
            target_hash_set = None

        batch_size = progress_interval
        current_idx = start_index

        while current_idx < end_index:
            batch_end = min(current_idx + batch_size, end_index)

            # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –ë–ï–ó –ü–†–û–í–ï–†–û–ö
            for idx in range(current_idx, batch_end):
                combination = self.index_to_combination(idx)

                # –í—ã—á–∏—Å–ª—è–µ–º —Ö–µ—à
                if self.hash_algo.startswith("wpa"):
                    if not self.ssid:
                        raise ValueError("SSID required for WPA/WPA2")
                    hash_bytes = HashAlgorithms.compute_wpa_psk(combination, self.ssid)
                else:
                    hash_bytes = HashAlgorithms.compute_hash(
                        combination.encode(),
                        self.hash_algo
                    )

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                if target_hash_set and hash_bytes in target_hash_set:
                    hash_hex = hash_bytes.hex()
                    solutions.append({
                        "combination": combination,
                        "hash": hash_hex,
                        "index": idx,
                        "mode": "brute"
                    })

                hash_count += 1

            current_idx = batch_end

            # –û—Ç—á–µ—Ç –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ
            if progress_callback:
                progress_callback(current_idx, hash_count)

        time_taken = time.time() - start_time
        hash_rate = hash_count / time_taken if time_taken > 0 else 0

        return {
            "hash_count": hash_count,
            "time_taken": time_taken,
            "hash_rate": hash_rate,
            "solutions": solutions,
            "start_index": start_index,
            "end_index": end_index,
            "mode": "brute",
            "workers_used": 1
        }

    def _compute_dictionary(
        self,
        start_index: int,
        end_index: int,
        target_hash: Optional[str],
        target_hashes: Optional[List[str]],
        progress_callback,
        progress_interval: int
    ) -> Dict[str, Any]:
        """Dictionary attack —Ä–µ–∂–∏–º —Å multiprocessing"""
        start_time = time.time()
        total_hash_count = 0
        all_solutions = []

        # Multi-target mode - –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ hex
        if target_hashes:
            target_hash_set_hex = target_hashes
        elif target_hash:
            target_hash_set_hex = [target_hash]
        else:
            target_hash_set_hex = None

        # –ü–æ–ª—É—á–∞–µ–º —Å–ª–∞–π—Å —Å–ª–æ–≤–∞—Ä—è
        wordlist_slice = self.wordlist[start_index:end_index]

        # –ï—Å–ª–∏ multiprocessing –æ—Ç–∫–ª—é—á–µ–Ω - single-threaded
        if not self.use_multiprocessing or len(wordlist_slice) < 100:
            return self._compute_dictionary_single(
                start_index, end_index, target_hash, target_hashes,
                progress_callback, progress_interval
            )

        # –ü–æ–ª—É—á–∞–µ–º pool
        pool = self._get_or_create_pool()
        if pool is None:
            return self._compute_dictionary_single(
                start_index, end_index, target_hash, target_hashes,
                progress_callback, progress_interval
            )

        # –†–∞–∑–±–∏–≤–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –Ω–∞ sub-chunks
        num_workers = pool._processes
        words_per_subchunk = max(100, len(wordlist_slice) // num_workers)

        tasks = []
        current = 0

        while current < len(wordlist_slice):
            subchunk_end = min(current + words_per_subchunk, len(wordlist_slice))
            words_subchunk = wordlist_slice[current:subchunk_end]

            task_args = (
                words_subchunk,
                self.mutations,
                self.hash_algo,
                self.ssid,
                target_hash_set_hex,
                start_index + current  # base index –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤
            )

            tasks.append(task_args)
            current = subchunk_end

        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
        results = pool.map(compute_dict_subchunk, tasks)

        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for solutions, hash_count in results:
            all_solutions.extend(solutions)
            total_hash_count += hash_count

            # –û—Ç—á–µ—Ç –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ
            if progress_callback:
                progress_callback(start_index + len(all_solutions), total_hash_count)

        time_taken = time.time() - start_time
        hash_rate = total_hash_count / time_taken if time_taken > 0 else 0

        return {
            "hash_count": total_hash_count,
            "time_taken": time_taken,
            "hash_rate": hash_rate,
            "solutions": all_solutions,
            "start_index": start_index,
            "end_index": end_index,
            "mode": "dictionary",
            "workers_used": num_workers
        }

    def _compute_dictionary_single(
        self,
        start_index: int,
        end_index: int,
        target_hash: Optional[str],
        target_hashes: Optional[List[str]],
        progress_callback,
        progress_interval: int
    ) -> Dict[str, Any]:
        """Dictionary attack single-threaded (fallback)"""
        start_time = time.time()
        hash_count = 0
        solutions = []

        # Multi-target mode
        if target_hashes:
            target_hash_set = {bytes.fromhex(h) for h in target_hashes}
        elif target_hash:
            target_hash_set = {bytes.fromhex(target_hash)}
        else:
            target_hash_set = None

        # –ü–æ–ª—É—á–∞–µ–º —Å–ª–∞–π—Å —Å–ª–æ–≤–∞—Ä—è
        wordlist_slice = self.wordlist[start_index:end_index]

        batch_size = progress_interval
        current_idx = 0

        while current_idx < len(wordlist_slice):
            batch_end = min(current_idx + batch_size, len(wordlist_slice))

            for idx in range(current_idx, batch_end):
                word = wordlist_slice[idx]

                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –º—É—Ç–∞—Ü–∏–∏
                if self.mutations:
                    candidates = self.mutation_engine.apply_mutations(word, self.mutations)
                else:
                    candidates = [word]

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–π –∫–∞–Ω–¥–∏–¥–∞—Ç
                for candidate in candidates:
                    # –í—ã—á–∏—Å–ª—è–µ–º —Ö–µ—à
                    if self.hash_algo.startswith("wpa"):
                        if not self.ssid:
                            raise ValueError("SSID required for WPA/WPA2")
                        hash_bytes = HashAlgorithms.compute_wpa_psk(candidate, self.ssid)
                    else:
                        hash_bytes = HashAlgorithms.compute_hash(
                            candidate.encode(),
                            self.hash_algo
                        )

                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                    if target_hash_set and hash_bytes in target_hash_set:
                        hash_hex = hash_bytes.hex()
                        solutions.append({
                            "combination": candidate,
                            "hash": hash_hex,
                            "index": start_index + idx,
                            "base_word": word,
                            "mode": "dictionary"
                        })

                    hash_count += 1

            current_idx = batch_end

            # –û—Ç—á–µ—Ç –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ
            if progress_callback:
                progress_callback(start_index + current_idx, hash_count)

        time_taken = time.time() - start_time
        hash_rate = hash_count / time_taken if time_taken > 0 else 0

        return {
            "hash_count": hash_count,
            "time_taken": time_taken,
            "hash_rate": hash_rate,
            "solutions": solutions,
            "start_index": start_index,
            "end_index": end_index,
            "mode": "dictionary",
            "workers_used": 1
        }


class Run(BaseService):
    SERVICE_NAME = "hash_worker"

    def __init__(self, service_name: str, proxy_client=None):
        super().__init__(service_name, proxy_client)
        self.info.version = "2.0.0"  # Updated with multiprocessing support
        self.info.description = "–í–æ—Ä–∫–µ—Ä —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π —Ö–µ—à–µ–π —Å multiprocessing"

        # –¢–µ–∫—É—â–∞—è –∑–∞–¥–∞—á–∞
        self.current_job_id: Optional[str] = None
        self.current_chunk_id: Optional[int] = None

        # –ú–µ—Ç—Ä–∏–∫–∏
        self.total_hashes_computed = 0
        self.completed_chunks = 0

        # –õ–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ (–¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏)
        self.processed_chunks: Dict[str, set] = {}  # {job_id: set(chunk_ids)}

        # Background tasks
        self.worker_task = None

        # –§–ª–∞–≥ —Ä–∞–±–æ—Ç—ã
        self.running = False

        # HashComputer instances (–¥–ª—è cleanup)
        self.active_computers: List[HashComputer] = []

        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è multiprocessing
        self.use_multiprocessing = True  # –ú–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å —á–µ—Ä–µ–∑ config
        self.max_workers = mp.cpu_count()
        self.max_cpu_percent = 80.0  # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º–æ–µ
        self.max_memory_percent = 80.0  # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º–æ–µ

    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞"""
        # –¢–æ–ª—å–∫–æ –Ω–∞ –≤–æ—Ä–∫–µ—Ä–∞—Ö
        if self.context.config.coordinator_mode:
            self.logger.info("Hash worker disabled on coordinator node")
            return

        self.logger.info("Hash worker initialized")
        self.running = True

        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª
        self.worker_task = asyncio.create_task(self._worker_loop())

    async def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        self.running = False
        if self.worker_task:
            self.worker_task.cancel()

        # Cleanup multiprocessing pools
        for computer in self.active_computers:
            try:
                computer.cleanup()
            except Exception as e:
                self.logger.error(f"Error cleaning up computer: {e}")

        self.active_computers.clear()

    async def _worker_loop(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –≤–æ—Ä–∫–µ—Ä–∞"""
        while self.running:
            try:
                await asyncio.sleep(0.1)  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–µ 5 —Å–µ–∫—É–Ω–¥

                # –ü–æ–ª—É—á–∞–µ–º –∞–∫—Ç–∏–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏ –∏–∑ gossip
                jobs = await self._get_active_jobs()

                found_work = False
                for job_id in jobs:
                    # –ü–æ–ª—É—á–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ —á–∞–Ω–∫–∏
                    chunk = await self._get_available_chunk(job_id)

                    if chunk:
                        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞–Ω–∫
                        await self._process_chunk(job_id, chunk)
                        found_work = True
                        break  # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ –æ–¥–Ω–æ–º—É —á–∞–Ω–∫—É –∑–∞ –∏—Ç–µ—Ä–∞—Ü–∏—é

                # –ï—Å–ª–∏ –Ω–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ - –¥–µ–ª–∞–µ–º –ø–∞—É–∑—É
                if not found_work:
                    self.logger.debug("No available chunks, waiting...")
                    await asyncio.sleep(10)  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø–∞—É–∑–∞ –µ—Å–ª–∏ –Ω–µ—Ç —Ä–∞–±–æ—Ç—ã

            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error(f"Error in worker loop: {e}")

    async def _get_active_jobs(self) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á –∏–∑ gossip"""
        network = self.context.get_shared("network")
        if not network:
            return []

        # –ò—â–µ–º hash_job_* –≤ metadata –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–∞
        coordinator_nodes = [
            node for node in network.gossip.node_registry.values()
            if node.role == "coordinator"
        ]

        if not coordinator_nodes:
            return []

        coordinator = coordinator_nodes[0]
        metadata = coordinator.metadata

        jobs = []
        for key in metadata.keys():
            if key.startswith("hash_job_"):
                job_id = key.replace("hash_job_", "")
                jobs.append(job_id)

        return jobs

    async def _get_available_chunk(self, job_id: str) -> Optional[dict]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã–π —á–∞–Ω–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑ gossip

        –õ–æ–≥–∏–∫–∞: –∫–∞–∂–¥–æ–º—É –≤–æ—Ä–∫–µ—Ä—É –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω —Å–≤–æ–π —á–∞–Ω–∫ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
        {ver: 5, chunks: {6: {assigned_worker: worker_id, ...}, 7: {...}}}
        """
        network = self.context.get_shared("network")
        if not network:
            return None

        # –ü–æ–ª—É—á–∞–µ–º –±–∞—Ç—á–∏ –∏–∑ gossip
        coordinator_nodes = [
            node for node in network.gossip.node_registry.values()
            if node.role == "coordinator"
        ]

        if not coordinator_nodes:
            return None

        coordinator = coordinator_nodes[0]
        metadata = coordinator.metadata

        batches_key = f"hash_batches_{job_id}"
        if batches_key not in metadata:
            return None

        batches = metadata[batches_key]

        # –í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º node_id –∏–∑ gossip (—Å –ø–æ—Ä—Ç–æ–º), –∞ –Ω–µ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        # –¢–∞–∫ –∫–∞–∫ coordinator –Ω–∞–∑–Ω–∞—á–∞–µ—Ç –±–∞—Ç—á–∏ –∏—Å–ø–æ–ª—å–∑—É—è gossip node_id
        network = self.context.get_shared("network")
        my_worker_id = network.gossip.node_id if network else self.context.config.node_id

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º set –¥–ª—è job_id –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
        if job_id not in self.processed_chunks:
            self.processed_chunks[job_id] = set()

        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –õ–æ–≥–∏—Ä—É–µ–º —á—Ç–æ –≤–∏–¥–∏–º –≤ gossip
        total_chunks = sum(len(batch_data.get("chunks", {})) for batch_data in batches.values())
        my_chunks = sum(
            1 for batch_data in batches.values()
            for chunk_data in batch_data.get("chunks", {}).values()
            if chunk_data.get("assigned_worker") == my_worker_id
        )
        self.logger.debug(f"üîç [DIAG] Checking batches for {job_id}: {len(batches)} versions, {total_chunks} total chunks, {my_chunks} assigned to me")

        # –ò—â–µ–º —á–∞–Ω–∫ –¥–ª—è –º–µ–Ω—è
        for version, batch_data in batches.items():
            chunks = batch_data.get("chunks", {})

            for chunk_id, chunk_data in chunks.items():
                if chunk_data.get("assigned_worker") == my_worker_id:
                    # –ü—Ä–∏–≤–æ–¥–∏–º chunk_id –∫ int –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                    chunk_id_int = int(chunk_id)

                    # –í–ê–ñ–ù–û: –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∏ (–ª–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à)
                    if chunk_id_int in self.processed_chunks[job_id]:
                        continue

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å
                    status = chunk_data.get("status", "assigned")

                    # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç—É—Å—ã –º–æ–∏—Ö —á–∞–Ω–∫–æ–≤
                    self.logger.debug(f"üîç [DIAG] My chunk {chunk_id_int}: status={status}, cached={chunk_id_int in self.processed_chunks[job_id]}")

                    if status in ("assigned", "recovery"):
                        # –≠—Ç–æ –º–æ–π —á–∞–Ω–∫, –º–æ–∂–Ω–æ –±—Ä–∞—Ç—å
                        self.logger.info(f"‚úÖ [DIAG] Found available chunk: {chunk_id_int} (status={status}, version={version})")
                        return {
                            "job_id": job_id,
                            "version": version,
                            "chunk_id": chunk_id_int,
                            "start_index": chunk_data["start_index"],
                            "end_index": chunk_data["end_index"],
                            "chunk_size": chunk_data["chunk_size"]
                        }

        return None

    async def _process_chunk(self, job_id: str, chunk: dict):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —á–∞–Ω–∫"""
        chunk_id = chunk["chunk_id"]
        start_index = chunk["start_index"]
        end_index = chunk["end_index"]

        self.logger.info(
            f"Processing chunk {chunk_id} for job {job_id}: "
            f"{start_index} - {end_index} ({chunk['chunk_size']} hashes)"
        )

        self.current_job_id = job_id
        self.current_chunk_id = chunk_id

        # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–¥–∞—á–∏
        job_metadata = await self._get_job_metadata(job_id)
        if not job_metadata:
            self.logger.error(f"Job metadata not found for {job_id}")
            return

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–¥–∞—á–∏
        mode = job_metadata.get("mode", "brute")
        hash_algo = job_metadata.get("hash_algo", "sha256")
        target_hash = job_metadata.get("target_hash")
        target_hashes = job_metadata.get("target_hashes")  # Multi-target mode

        # –°–æ–∑–¥–∞–µ–º –∫–æ–º–ø—å—é—Ç–µ—Ä –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞ —Å multiprocessing support
        if mode == "brute":
            charset = job_metadata["charset"]
            length = job_metadata["length"]
            ssid = job_metadata.get("ssid")  # –î–ª—è WPA/WPA2

            computer = HashComputer(
                charset=charset,
                length=length,
                hash_algo=hash_algo,
                mode="brute",
                ssid=ssid,
                use_multiprocessing=self.use_multiprocessing,
                max_workers=self.max_workers,
                max_cpu_percent=self.max_cpu_percent,
                max_memory_percent=self.max_memory_percent
            )

        elif mode == "dictionary":
            wordlist = job_metadata.get("wordlist", [])
            mutations = job_metadata.get("mutations", [])
            ssid = job_metadata.get("ssid")

            computer = HashComputer(
                hash_algo=hash_algo,
                mode="dictionary",
                wordlist=wordlist,
                mutations=mutations,
                ssid=ssid,
                use_multiprocessing=self.use_multiprocessing,
                max_workers=self.max_workers,
                max_cpu_percent=self.max_cpu_percent,
                max_memory_percent=self.max_memory_percent
            )

        else:
            self.logger.error(f"Unknown mode: {mode}")
            return

        # –î–æ–±–∞–≤–ª—è–µ–º –≤ active computers –¥–ª—è cleanup
        self.active_computers.append(computer)

        # –ü—É–±–ª–∏–∫—É–µ–º —Å—Ç–∞—Ç—É—Å "working" –≤ gossip
        await self._publish_chunk_status(job_id, chunk_id, "working", start_index)

        # Callback –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        last_progress_time = time.time()

        def progress_callback(current_idx, hash_count):
            nonlocal last_progress_time
            now = time.time()

            # –ü—É–±–ª–∏–∫—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å —Ä–∞–∑ –≤ 10 —Å–µ–∫—É–Ω–¥
            if now - last_progress_time >= 10:
                asyncio.create_task(
                    self._publish_chunk_progress(job_id, chunk_id, current_idx)
                )
                last_progress_time = now

        # –í—ã—á–∏—Å–ª—è–µ–º!
        start_time = time.time()
        result = computer.compute_chunk(
            start_index,
            end_index,
            target_hash=target_hash,
            target_hashes=target_hashes,
            progress_callback=progress_callback,
            progress_interval=10000
        )
        time_taken = time.time() - start_time

        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        self.total_hashes_computed += result["hash_count"]
        self.completed_chunks += 1

        # –ü—É–±–ª–∏–∫—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        await self._publish_chunk_completed(
            job_id,
            chunk_id,
            result["hash_count"],
            time_taken,
            result["solutions"]
        )

        # –ü–æ–ª—É—á–∞–µ–º system load –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        system_load = computer.system_monitor.get_current_load()
        workers_used = result.get("workers_used", 1)

        self.logger.info(
            f"Completed chunk {chunk_id}: {result['hash_count']} hashes in "
            f"{time_taken:.2f}s ({result['hash_rate']:.0f} h/s) | "
            f"Workers: {workers_used}/{mp.cpu_count()} | "
            f"CPU: {system_load['cpu_percent']:.1f}% | "
            f"Memory: {system_load['memory_percent']:.1f}%"
        )

        if result["solutions"]:
            self.logger.warning(f"FOUND {len(result['solutions'])} SOLUTIONS!")
            for sol in result["solutions"]:
                self.logger.warning(f"Solution: {sol['combination']} ‚Üí {sol['hash']}")

            # –ù–µ–º–µ–¥–ª–µ–Ω–Ω–æ —É–≤–µ–¥–æ–º–ª—è–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä –æ –Ω–∞—Ö–æ–¥–∫–µ —á–µ—Ä–µ–∑ RPC
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º node_id –∏–∑ gossip (—Å –ø–æ—Ä—Ç–æ–º)
                network = self.context.get_shared("network")
                worker_id = network.gossip.node_id if network else self.context.config.node_id

                await self.proxy.hash_coordinator.report_solution(
                    job_id=job_id,
                    chunk_id=chunk_id,
                    worker_id=worker_id,
                    solutions=result["solutions"]
                )
                self.logger.info(f"Reported {len(result['solutions'])} solutions to coordinator")
            except Exception as e:
                self.logger.error(f"Failed to report solutions to coordinator: {e}")

        # Cleanup computer –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è chunk
        try:
            computer.cleanup()
            self.active_computers.remove(computer)
        except Exception as e:
            self.logger.error(f"Error cleaning up computer: {e}")

        self.current_job_id = None
        self.current_chunk_id = None

    async def _get_job_metadata(self, job_id: str) -> Optional[dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ –∏–∑ gossip"""
        network = self.context.get_shared("network")
        if not network:
            return None

        coordinator_nodes = [
            node for node in network.gossip.node_registry.values()
            if node.role == "coordinator"
        ]

        if not coordinator_nodes:
            return None

        coordinator = coordinator_nodes[0]
        metadata = coordinator.metadata

        job_key = f"hash_job_{job_id}"
        return metadata.get(job_key)

    async def _publish_chunk_status(
        self,
        job_id: str,
        chunk_id: int,
        status: str,
        progress: int
    ):
        """–ü—É–±–ª–∏–∫—É–µ—Ç —Å—Ç–∞—Ç—É—Å —á–∞–Ω–∫–∞ –≤ gossip"""
        network = self.context.get_shared("network")
        if not network:
            return

        worker_status = {
            "job_id": job_id,
            "chunk_id": chunk_id,
            "status": status,
            "progress": progress,
            "timestamp": time.time(),
            "total_hashes": self.total_hashes_computed,
            "completed_chunks": self.completed_chunks
        }

        # Use new versioned update_metadata API
        network.gossip.update_metadata("hash_worker_status", worker_status)

    async def _publish_chunk_progress(
        self,
        job_id: str,
        chunk_id: int,
        current_index: int
    ):
        """–ü—É–±–ª–∏–∫—É–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ gossip"""
        await self._publish_chunk_status(job_id, chunk_id, "working", current_index)

    async def _publish_chunk_completed(
        self,
        job_id: str,
        chunk_id: int,
        hash_count: int,
        time_taken: float,
        solutions: List[dict]
    ):
        """–ü—É–±–ª–∏–∫—É–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —á–∞–Ω–∫–∞ –≤ gossip –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä—É —á–µ—Ä–µ–∑ RPC"""
        network = self.context.get_shared("network")
        if not network:
            return

        # 1. –ü—É–±–ª–∏–∫—É–µ–º –≤ gossip (–¥–ª—è dashboard –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è)
        worker_status = {
            "job_id": job_id,
            "chunk_id": chunk_id,
            "status": "solved",
            "hash_count": hash_count,
            "time_taken": time_taken,
            "solutions": solutions,
            "timestamp": time.time(),
            "total_hashes": self.total_hashes_computed,
            "completed_chunks": self.completed_chunks
        }

        # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –õ–æ–≥–∏—Ä—É–µ–º –ø—É–±–ª–∏–∫–∞—Ü–∏—é —Å—Ç–∞—Ç—É—Å–∞
        self.logger.info(f"üì§ [DIAG] Publishing chunk completed: chunk_id={chunk_id}, status=solved, hash_count={hash_count}")

        # Use new versioned update_metadata API
        network.gossip.update_metadata("hash_worker_status", worker_status)

        # –î–æ–±–∞–≤–ª—è–µ–º chunk_id –≤ –ª–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        if job_id not in self.processed_chunks:
            self.processed_chunks[job_id] = set()
        self.processed_chunks[job_id].add(chunk_id)

        self.logger.info(f"‚úÖ [DIAG] Marked chunk {chunk_id} as processed in local cache for job {job_id}")

        # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä –ø—Ä–æ—á–∏—Ç–∞–µ—Ç —ç—Ç–æ—Ç —Å—Ç–∞—Ç—É—Å –∏–∑ gossip –≤ _update_worker_states()

    @service_method(description="–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –≤–æ—Ä–∫–µ—Ä–∞", public=True)
    async def get_worker_status(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å –≤–æ—Ä–∫–µ—Ä–∞"""
        return {
            "success": True,
            "current_job": self.current_job_id,
            "current_chunk": self.current_chunk_id,
            "total_hashes_computed": self.total_hashes_computed,
            "completed_chunks": self.completed_chunks,
            "running": self.running
        }
